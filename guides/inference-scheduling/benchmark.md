# Well-lit Path: Intelligent Inference Scheduling -- Benchmark

## Overview

For full, customizable benchmark, please refer to [llm-d-benchmark](https://github.com/llm-d/llm-d-benchmark). This document describes how to run benchmarks against an Intelligent Inference Scheduling llm-d stack that is already created. 

## Requirements

- **yq**: install version>=4
- **run_only.sh**: [run_only.sh](https://raw.githubusercontent.com/dmitripikus/llm-d-benchmark/refs/heads/run_only/existing_stack/run_only.sh) - the main script
- **function.sh**: [functions.sh](https://raw.githubusercontent.com/dmitripikus/llm-d-benchmark/refs/heads/run_only/existing_stack/functions.sh)
- **config_template.yaml**: [config_template.yaml](https://raw.githubusercontent.com/dmitripikus/llm-d-benchmark/refs/heads/run_only/existing_stack/config_template.yaml) - configuration

## Setup the required environment variables

  ```bash
  export NAMESPACE=<Your namespace>
  export GATEWAY_SVC=$(
    kubectl get svc \
    -l app.kubernetes.io/gateway=infra-inference-scheduling-inference-gateway \
    --no-headers  -o=custom-columns=:metadata.name
  )
  ```

## Create a PVC to save your benchmark results

E.g., `workload-pvc`

> TODO: add instructions on how to create a PVC. Must be RWX.

## Create a yaml configuration file for the benchmark

  ```bash
  envsusbt <config_template.yaml > config.yaml
  ```

## Run

  ```bash
  ./run_only.sh -c config.yaml
  ```

The benchmarks will run and the resulted would be stored on the PVC.

## Analyze Results

TBD

---

# Advanced

## Customizing the config file

This section describes the details of the configuration `config.yaml` file. You may edit it as needed to match your stack (e.g., to change the model name). If you followed the guideline to create your stack then you should be able to run without any changes.   
**Do not change** unless you know what you are doing.

The configuration is divided into sections, each with a different scope.

### Endpoint

These are the properties of the stack (`envsubst` would replace `NAMESPACE` and `GATEWAY_SVC` to match your env). Gated models need a Hugging Face token to access. Your stack should already have a token secret under the name `llm-d-hf-token`. `stack_name` is a user-defined arbitrary name that will be attached to the benchmark results. You can use `stack_name` to help you identify the results of different experiments. The `model` must match your stack. Please note the `yaml` tags -- other section of this `yaml` reference them (e.g., the tokenizer should reference the model).   
  ```yaml
  endpoint:
    stack_name: &stack_name inference-scheduling-Qwen3-0.6B  # user defined name for the stack (results prefix)
    model: &model Qwen/Qwen3-0.6B                      # Exact HuggingFace model name. Must match stack deployed.
    namespace: &namespace $NAMESPACE
    base_url: &url http://${GATEWAY_SVC}.${NAMESPACE}.svc.cluster.local:80  # Base URL of inference endpoint
    hf_token_secret: llm-d-hf-token   # The name of secret that contains the HF token of the stack
  ```

### Control

These define the local target directory for temporary files and for fetching results.
The `kubectl` property allows you to change the k8s control command (e.g., to `oc`).

  ```yaml
  control:
    work_dir: $HOME/llm-d-bench-work # working directory to store temporary and autogenerated files. 
                                      # Do not edit content manually.
                                      # If not set, a temp directory will be created.
    kubectl: kubectl                  # kubectl command: kubectl or oc                                   
  ```

### Harness

Harness refers to the specific benchmarking tool used. Several harnesses are supported, including [inference-perf](https://github.com/kubernetes-sigs/inference-perf), [guidellm](https://github.com/vllm-project/guidellm), [InferenceMAX](https://github.com/InferenceMAX/InferenceMAX) and [vLLM Benchmarks](https://github.com/vllm-project/vllm/tree/main/benchmarks). The `results_pvc` should be set to the PVC you created on the previous step. The benchmark is run from one or more pods inside the cluster. The image for this pod is from [llm-d-benchmark](https://github.com/llm-d/llm-d-benchmark). Typically, you do not have to change the `namespace` or the `image` 


The benchmarking harness refers to the be 

  ```yaml
  harness:
    name: &harness_name inference-perf
    results_pvc: workload-pvc       # PVC where benchmark results are stored
    namespace: *namespace           # Namespace where harness is deployed. Typically with stack.
    parallelism: 1                  # Number of parallel workload launcher pods to create.  
    wait_timeout: 600               # Time (in seconds) to wait for workload launcher pod to complete before terminating.
                                    # Set to 0 to disable timeout.
                                    # Note: workload launcher pod will continue running in cluster if timeout occurs.
    image: ghcr.io/llm-d/llm-d-benchmark:v0.3.7
    # dataset_url: &dataset_url none
    # dataset_path: &dataset_path none                                
  ```

### Extra environment variables

This sections allows you to add arbitrary environment variable to the harness pod. This is mostly useful to change behavior for a specific harness. For example, change the parallelism level.

  ```yaml
  env:
    - name: RAYON_NUM_THREADS
      value: "4"
  ```

### Workload 

This configure the characteristics of the workload used to benchmark the stack. Each harness supports different configuration parameters for setting the workload. These are described in detail in their documentations (see, e.g., [inference-perf configuration guide](https://github.com/kubernetes-sigs/inference-perf/blob/main/docs/config.md)).
While the details are different for each harness, a workload specification typically includes:
 - **Data specification**: How to generated the contents of the inference queries. For example, the distribution of input and output lengths or a HF trace.   
 - **Load specification**: Timing for sending queries. E.g., rate and duration. Some harnesses support "stages", each with its own load specification.
 - **Extra**: Which API to use, target endpoint, tokenizers, etc.
 - **Output**: The types of reports to produce and where to store them. **Do not change** -- the benchmark tools will set these automatically. 

Several workload can be specified, each with a different name. The benchmark would run all the workloads against the stack. 

  ```yaml
  workload:                         # yaml configuration for harness workload(s)
    
    # an example workload using random synthetic data
    sanity_random:
      load:
        type: constant
        stages:
        - rate: 1
          duration: 30
      api:
        type: completion
        streaming: true
      server:
        type: vllm
        model_name: *model
        base_url: *url
        ignore_eos: true
      tokenizer:
        pretrained_model_name_or_path: *model
      data:
        type: random
        input_distribution:
          min: 10             # min length of the synthetic prompts
          max: 100            # max length of the synthetic prompts
          mean: 50            # mean length of the synthetic prompts
          std: 10             # standard deviation of the length of the synthetic prompts
          total_count: 100    # total number of prompts to generate to fit the above mentioned distribution constraints
        output_distribution:
          min: 10             # min length of the output to be generated
          max: 100            # max length of the output to be generated
          mean: 50            # mean length of the output to be generated
          std: 10             # standard deviation of the length of the output to be generated
          total_count: 100    # total number of output lengths to generate to fit the above mentioned distribution constraints
        # dataset_url: https://huggingface.co/datasets/deanli/llm-d-benchmark-datasets/resolve/main/small_instruction_following_dataset.jsonl
      report:
        request_lifecycle:
          summary: true
          per_stage: true
          per_request: true
      storage:
        local_storage:
          path: /workspace

    # an example workload using shared prefix synthetic data
    shared_prefix_synthetic:
      load:
        type: constant
        stages:
        - rate: 2
          duration: 40
        - rate: 5
          duration: 50
        - rate: 8
          duration: 60
      api:
        type: completion
        streaming: true
      server:
        type: vllm
        model_name: *model
        base_url: *url
        ignore_eos: true
      tokenizer:
        pretrained_model_name_or_path: *model
      data:
        type: shared_prefix
        shared_prefix:
          num_groups: 32                # Number of distinct shared prefixes
          num_prompts_per_group: 32     # Number of unique questions per shared prefix
          system_prompt_len: 2048       # Length of the shared prefix (in tokens)
          question_len: 256             # Length of the unique question part (in tokens)
          output_len: 256               # Target length for the model's generated output (in tokens)
      report:
        request_lifecycle:
          summary: true
          per_stage: true
          per_request: true
      storage:
        local_storage:
          path: /workspace
  ```
