endpoint:
  stack_name: &stack_name inf-scheduling-Qwen3-32B  # user defined name for the stack (results prefix)
  model: &model Qwen/Qwen3-32B        # Exact HuggingFace model name. Must match stack deployed.
  namespace: &namespace ${NAMESPACE}  # Namespace where stack is deployed
  base_url: &url http://${GATEWAY_SVC}.${NAMESPACE}.svc.cluster.local:80  # Base URL of inference endpoint
  hf_token_secret: llm-d-hf-token     # The name of secret that contains the HF token of the stack

control:
  work_dir: $HOME/llm-d-bench-work  # working directory to store temporary and autogenerated files. 
                                    # Do not edit content manually.
                                    # If not set, a temp directory will be created.
  kubectl: kubectl                  # kubectl command: kubectl or oc                                   

harness:
  name: &harness_name inference-perf
  results_pvc: ${BENCHMARK_PVC}   # PVC where benchmark results are stored
  namespace: *namespace           # Namespace where harness is deployed. Typically with stack.
  parallelism: 1                  # Number of parallel workload launcher pods to create.  
  wait_timeout: 6000              # Time (in seconds) to wait for workload launcher pod to complete before terminating.
                                  # Set to 0 to disable timeout.
  image: ghcr.io/llm-d/llm-d-benchmark:v0.4.0
  # dataset_url: &dataset_url none
                         
env:
  - name: RAYON_NUM_THREADS
    value: "4"

workload:                         # yaml configuration for harness workload(s)
  
  # Workload A: High System Prompt to User Prompt ratio, No System Cache Pressure",
  A-High-ratio-No-Pressure: 
    load:
      type: poisson
      interval: 15
      sweep:                        # Automatically determine saturation point of the target service and generate stages
        type: linear                # Produce a linear distribution [1.0, saturation] of rates for num_stages or geometric distribution clustered around the saturation point
        timeout: 60                 # Length of time to run load to determine saturation
        num_stages: 7               # Number of stages to generate
        stage_duration: 180         # Duration of each generated stage
        saturation_percentile: 95
    api:
      type: chat
      streaming: true
    server:
      type: vllm
      model_name: *model
      base_url: *url
      ignore_eos: true
    tokenizer:
      pretrained_model_name_or_path: *model
    data:
      type: shared_prefix
      shared_prefix:
        num_groups: 6                 # Number of distinct prefix, Note: the number of users is num_groups * num_prompts_per_group
        num_prompts_per_group: 1000   # Number of unique questions per group (prefix)
        system_prompt_len: 1000       # Length of the first prefix (in tokens), simulate initialization of a system prompt
        question_len: 30              # Length of the unique question part (in tokens)
        #question_len_std=9
        output_len: 1000              # Target length for the model's generated output (in tokens)
        #output_len_std=300
        enable_multi_turn_chat: true  # enable multi-turn chat, it will create user session to keep the conversation. The chat context will be appended for the each request.
    report:
      request_lifecycle:
        summary: true
        per_stage: true
        per_request: true
    storage:
      local_storage:
        path: /workspace

  # Workload B: Low System Prompt to User Prompt ratio, No System Cache Pressure 
  B-Low-ratio-No-Pressure: 
    load:
      type: poisson
      interval: 15
      sweep:                        # Automatically determine saturation point of the target service and generate stages
        type: linear                # Produce a linear distribution [1.0, saturation] of rates for num_stages or geometric distribution clustered around the saturation point
        timeout: 60                 # Length of time to run load to determine saturation
        num_stages: 7               # Number of stages to generate
        stage_duration: 180         # Duration of each generated stage
        saturation_percentile: 95
    api:
      type: chat
      streaming: true
    server:
      type: vllm
      model_name: *model
      base_url: *url
      ignore_eos: true
    tokenizer:
      pretrained_model_name_or_path: *model
    data:
      type: shared_prefix
      shared_prefix:
        num_groups: 6                 # Number of distinct prefix, Note: the number of users is num_groups * num_prompts_per_group
        num_prompts_per_group: 1000   # Number of unique questions per group (prefix)
        system_prompt_len: 1000       # Length of the first prefix (in tokens), simulate initialization of a system prompt
        question_len: 3000            # Length of the unique question part (in tokens)
        #question_len_std=900
        output_len: 1000              # Target length for the model's generated output (in tokens)
        #output_len_std=300
        enable_multi_turn_chat: true  # enable multi-turn chat, it will create user session to keep the conversation. The chat context will be appended for the each request.
    report:
      request_lifecycle:
        summary: true
        per_stage: true
        per_request: true
    storage:
      local_storage:
        path: /workspace

  # C: High System Prompt to User Prompt ratio, High System Cache Pressure",
  C-High-ratio-High-Pressure: 
    load:
      type: poisson
      interval: 15
      sweep:                        # Automatically determine saturation point of the target service and generate stages
        type: linear                # Produce a linear distribution [1.0, saturation] of rates for num_stages or geometric distribution clustered around the saturation point
        timeout: 60                 # Length of time to run load to determine saturation
        num_stages: 7               # Number of stages to generate
        stage_duration: 180         # Duration of each generated stage
        saturation_percentile: 95
    api:
      type: chat
      streaming: true
    server:
      type: vllm
      model_name: *model
      base_url: *url
      ignore_eos: true
    tokenizer:
      pretrained_model_name_or_path: *model
    data:
      type: shared_prefix
      shared_prefix:
        num_groups: 150               # Number of distinct prefix, Note: the number of users is num_groups * num_prompts_per_group
        num_prompts_per_group: 5      # Number of unique questions per group (prefix)
        system_prompt_len: 6000       # Length of the first prefix (in tokens), simulate initialization of a system prompt
        question_len: 1200            # Length of the unique question part (in tokens)
        #question_len_std=360
        output_len: 1000              # Target length for the model's generated output (in tokens)
        #output_len_std=300
        enable_multi_turn_chat: true  # enable multi-turn chat, it will create user session to keep the conversation. The chat context will be appended for the each request.
    report:
      request_lifecycle:
        summary: true
        per_stage: true
        per_request: true
    storage:
      local_storage:
        path: /workspace

  # D: Low System Prompt to User Prompt ratio, Low System Cache Pressure",
  D-Low-ratio-Low-Pressure: 
    load:
      type: poisson
      interval: 15
      sweep:                        # Automatically determine saturation point of the target service and generate stages
        type: linear                # Produce a linear distribution [1.0, saturation] of rates for num_stages or geometric distribution clustered around the saturation point
        timeout: 60                 # Length of time to run load to determine saturation
        num_stages: 7               # Number of stages to generate
        stage_duration: 180         # Duration of each generated stage
        saturation_percentile: 95
    api:
      type: chat
      streaming: true
    server:
      type: vllm
      model_name: *model
      base_url: *url
      ignore_eos: true
    tokenizer:
      pretrained_model_name_or_path: *model
    data:
      type: shared_prefix
      shared_prefix:
        num_groups: 150               # Number of distinct prefix, Note: the number of users is num_groups * num_prompts_per_group
        num_prompts_per_group: 5      # Number of unique questions per group (prefix)
        system_prompt_len: 1000       # Length of the first prefix (in tokens), simulate initialization of a system prompt
        question_len: 7200            # Length of the unique question part (in tokens)
        #question_len_std=2160,
        output_len: 1000              # Target length for the model's generated output (in tokens)
        #output_len_std=300,
        enable_multi_turn_chat: true  # enable multi-turn chat, it will create user session to keep the conversation. The chat context will be appended for the each request.
    report:
      request_lifecycle:
        summary: true
        per_stage: true
        per_request: true
    storage:
      local_storage:
        path: /workspace

  # an example workload using random synthetic data
  # sanity_random:
  #   load:
  #     type: constant
  #     stages:
  #     - rate: 1
  #       duration: 30
  #   api:
  #     type: completion
  #     streaming: true
  #   server:
  #     type: vllm
  #     model_name: *model
  #     base_url: *url
  #     ignore_eos: true
  #   tokenizer:
  #     pretrained_model_name_or_path: *model
  #   data:
  #     type: random
  #     input_distribution:
  #       min: 10             # min length of the synthetic prompts
  #       max: 100            # max length of the synthetic prompts
  #       mean: 50            # mean length of the synthetic prompts
  #       std: 10             # standard deviation of the length of the synthetic prompts
  #       total_count: 100    # total number of prompts to generate to fit the above mentioned distribution constraints
  #     output_distribution:
  #       min: 10             # min length of the output to be generated
  #       max: 100            # max length of the output to be generated
  #       mean: 50            # mean length of the output to be generated
  #       std: 10             # standard deviation of the length of the output to be generated
  #       total_count: 100    # total number of output lengths to generate to fit the above mentioned distribution constraints
  #   report:
  #     request_lifecycle:
  #       summary: true
  #       per_stage: true
  #       per_request: true
  #   storage:
  #     local_storage:
  #       path: /workspace
